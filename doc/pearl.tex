\documentclass{article}

\usepackage{todonotes}

\usepackage[authoryear]{natbib}
\bibliographystyle{apalike}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}

% grab parts of other files
\usepackage{catchfilebetweentags}
\input{robust-catch}

% tree diagrams
\usepackage{tikz}
\usetikzlibrary{matrix}

% algorithm
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

%haskell
\usepackage{minted}

% parens around citeyear
\newcommand{\parenciteyear}[1]{(\citeyear{#1})}

\usepackage{cleveref}

% unumbered remarks
\usepackage{amsthm}
\newtheorem*{remark}{Remark}


\title{Certified Binary Search in a Read-Only Array}
\author{Guillaume ALLAIS}

\begin{document}
\maketitle

\begin{abstract}
  Reifying the control flow of imperative programs as data structures allows
  functional programmers to implement common algorithms by an obviously
  terminating structural recursion. Additionally, this inductive structure
  can be decorated with invariants allowing the procedure to be proven correct.

  This ease of definition and certification comes at a cost because the
  functional data structure is typically not stored as efficiently in
  memory as the imperative counterpart it replaces.

  In this paper we use quantitative type theory as implemented in Idris2 to
  combine an efficient runtime representation of the data with a convenient
  compile time inductive view of it. This grants us ease of definition and
  certification at no runtime cost.
\end{abstract}

\section{Introduction}

One of the strengths of functional programming is the ability to reify common
control flow structures as inductive types and replace the corresponding ad-hoc
iteration constructs by recursive functions consuming these inductively defined
values.

This is perhaps best exemplified by the \texttt{forM} construct which provides
users with a convenient replacement for a \texttt{for} loop iterating over a
range of values.
%
Instead of writing \texttt{for(i = 0; i <= 10; i=i+2) body} to execute
\texttt{body} with \texttt{i} taking the successive values $0$, $2$, etc. up to
$10$, the user can first write \texttt{[0,2..10]} to generate a list
corresponding to the range of interest and then iterate over it with
\texttt{forM} thus getting the equivalent program
\texttt{forM [0,2..10] \$ \textbackslash i -> body}.

Replacing equals for equals is of course not the most exciting.
%
A first comparative advantage of the functional approach is the fact that any
function or system call producing a list of values (not just integers!) can now
become a source of control flow.
%
The second and perhaps more important one comes from the fact that these
constructs can then be extended to work with a whole \emph{class} of inductive
types thus generalising the original iteration principle.
%
In Haskell's standard library, \texttt{forM} will for instance work over not
just lists but more generally any \texttt{Traversable}
type~\citep{DBLP:journals/jfp/McbrideP08},
that is to say any finitely branching tree-like structure.

This generalisation can unfortunately come at a cost. If Haskell's optimiser is
good enough to generate tight loops for programs similar to the one above, it
will not be able to avoid the extra allocations and pointer chasing introduced by
the use of an inductive data type in all cases. We will consider one such case in
\cref{example:search}.

The work on LoCal, a functional language whose types carry information about the
runtime data-layout of the recursive structures being traversed, attempts to solve
that problem. The careful typing rules ensure the user-written programs manipulating
inductive data can be run directly on its serialised form.
%
In it, \citet{DBLP:conf/pldi/VollmerKRS0N19} warn against the explicit manipulation
of pointers (what they call Cursor is essentially a type-informed offset of
a pointer)\todo{double check}.
%
This justifies their approach: adding built-in compiler support.

\begin{quote}
  Cursors need to be manipulated carefully to visit the necessary portions
  of the buffer skipping over the sections that are not needed and read out
  the appropriate data, all without the safety net of a type checker. Hence,
  writing code to work directly on the serialized data can be tedious and
  error-prone.
\end{quote}

As we are going to see in this paper, from the point of view of the dependently
typed programmer this special built-in support is redundant.
%
In the same way that data-generic programming is usual programming over a closed
universe of descriptions of data types~\citep{DBLP:conf/ifip2-1/AltenkirchM02},
programming over serialised data is usual programming over data that is aware
of its serialisation format.

\subsection{Roadmap}\todo{expand}

In this pearl we will focus on a specific algorithm: binary search in a
sorted read-only array. This is typically used to efficiently check whether
a given value belongs to a set.

\paragraph{Section \ref{sec:reconstruct}} is dedicated to rediscovering the
underlying structure of search by dichotomy. It leads us to the corresponding
functional abstraction using a binary search tree.

\paragraph{Section \ref{sec:qtt}} is a quick introduction to the quantitative
type theory underpinning Idris2 and the notion of runtime irrelevance that it
provides.

\paragraph{Section \ref{sec:treearray}} is the core of our pearl: the
runtime irrelevant inductive structure that allows us to see a read-only array
as a tree in disguise.
%
All of the invariant-enforcing information the runtime irrelevant structure
carries allows us to prove a logarithmic \emph{decidability} principle for
array membership.


\section{Search by dichotomy}
\label{sec:reconstruct}

Binary search is a classic example of an algorithm offering an asymptotic
improvement over a naïve approach, under the assumption that the input data
is structured.
%
Provided a set represented as an array that is known to be sorted, it is
possible to test set membership in logarithmic time instead of taking linear
time to compare the value of interest to each value in the set.

The key observation underpinning this algorithm is that, by virtue of the array
being sorted, the value in the middle of any subarray divides it in two parts:
the left one, only containing strictly smaller values,
and the right one, only containing strictly larger ones.
%
Using this knowledge, at each step of the procedure we can eliminate half of
the remaining candidates. Comparing the value in the middle of the subarray
we are focused on with our value of interest, we can
either stop if we have successfully found our target
or focus on the left or the right half of the subarray depending on whether
our target is smaller or larger than the value we just read.

Let us look at a couple of executions of this search algorithm on a set
represented as a sorted array.

\subsection{Search, by example}
\label{example:search}

In figure~\ref{fig:searchsuccess} we are looking for
the value 11 in a sorted array which happens to contain the first
ten prime numbers.
%
We start by reading the value in the middle of the whole array, find out it is
13 which is larger than 11 and thus focus on the left half of the array for the
next step.
%
We then read the value in the middle of this left half, obtain 5
which is smaller than 11 and thus shift our focus to the right of 5, that is
to say on the second quarter of the whole array.
%
After reading 7, we shift once more our focus to the right and finally manage
to find our target. The search for 11 was successful.

\begin{figure}[h]
  \center
  \ExecuteMetaData[tree.tex]{search11}
  \caption{Tracing the binary search for 11}
  \label{fig:searchsuccess}
\end{figure}

If we had been looking for the value 31 instead, we would have obtained the
unsuccessful trace shown in figure~\ref{fig:searchfail}. We start from the
middle of the array and repeatedly shift our focus to the right because we
keep reading numbers smaller than our target. We eventually reach the end of
the array and can declare that 31 is not a member of the set.


\begin{figure}[h]
  \center
  \ExecuteMetaData[tree.tex]{search31}
  \caption{Tracing the binary search for 31}
  \label{fig:searchfail}
\end{figure}

Now that we have refreshed our memory and have hopefully convinced ourselves
that this is indeed a correct algorithm, let us look at its formal definition.

\subsection{Search, imperatively}

The procedure relies on three local variables.
%
The first one (\textit{found}) is a boolean that corresponds to whether we
have found our needle yet.
%
The two others (\textit{begin} and \textit{end} respectively) are integers
tracking the boundaries of the subarray of interest. They are initialised to
zero and the array's size respectively.
%
Throughout the search it should always be the case that all of the integers
between \textit{begin} (inclusive) and \textit{end} (exclusive) are valid
indices of our input array.

The core of the procedure is expressed as a \texttt{while} loop: as long
as we have not yet found our candidate, and the subarray in focus is non-empty
(i.e. \textit{begin} is strictly smaller than \textit{end}) then we look
at the value in the middle of the subarray and compare it to our
\textit{needle}.

If it is equal, we set \textit{found} to true and will promptly exit and
report this good news.
%
If it smaller then we focus on the left half of the subarray i.e. we keep
the beginning the same but use \textit{middle} as our new endpoint.
%
Otherwise it is larger and we focus on the right half of the subarray i.e.
we keep the end the same but shift the beginning to the index immediately
to the right of the \textit{middle}.

\input{algo}

This listing gives us a formal definition of the algorithm. It should be
enough to prove it solves the set membership decision problem under the
assumption that the input array is sorted.
%
During such a proof we need to ensure that

\begin{enumerate}
\item the algorithm terminates by proving that the size of the subarray in focus
($\mathit{end} - \mathit{begin}$) is a strictly decreasing natural number

\item all of our array reads are valid i.e. that \textit{middle} is always a
  valid index (in particular its computation never overflows)

\item the end value of \textit{found} is the correct answer to the decision
  problem (this relies on the assumption that the array is sorted).
\end{enumerate}

We ought to be able to use a specification language for imperative algorithms
and interactive or automated provers \todo{cite Dafny, Why, etc?} to check
our reasoning.

\subsection{The computation's underlying structure}

Despite now having a formal definition of the algorithm,
we have not shone any light on the structure of the computation.
If a list of values is the essence of a \texttt{for} loop,
what is that of a search by dichotomy?
%
This can be answered by going back to our running example and taking the
time to observe all possible behaviours of binary search on our array of
prime numbers.

In figure \ref{fig:searchall} we trace all of the possible execution paths
and see a tree revealed.
%
This is the underlying structure of the binary search computation: a binary
search tree! Additionally, because the procedure is guaranteed to halve the
number of candidates at each step, we know that this binary search tree is
balanced.\todo{Be precise: AVL?}

\begin{figure}[h]
  \center
  \ExecuteMetaData[tree.tex]{searchall}
  \caption{Tracing all possible binary searches}
  \label{fig:searchall}
\end{figure}

This suggests a straightforward avenue for implementation in a functional
language or formalisation in a proof assistant: ditch the array and use the
underlying binary search tree instead.
%
This has the added benefit of replacing an unwieldy induction over the size
of the subarray in focus in favour of a purely structural one on the tree.

Additionally, in a dependently typed language we can annotate the tree with
invariants thus guaranteeing that the values are indeed
sorted~\citep{DBLP:conf/icfp/McBride14}.
%
This empowers
us to not only write a search procedure but also prove that it decides the
set membership problem.

However this would have adverse consequences in terms of performance: instead of
a compact representation using a contiguous array, we now have a tree whose
nodes comprise of a value and two pointers to its children.

Can we have the best of both worlds? A compact runtime representation using an
array and an obviously terminating recursive decision procedure?
%
We will decidedly answer yes in section~\ref{sec:treearray} thanks to the tools
we are going to introduce now.


\section{QTT and Runtime Irrelevance}
\label{sec:qtt}

\subsection{Example: a Safe Open Union Type}

In their work on extensible effects
in Haskell~\parenciteyear{DBLP:conf/haskell/KiselyovI15}
Kiselyov and Ishii use an open union type to efficiently represent the
sum of all of the effects available in a given computation.

Internally this open union type is a Generalised Algebraic Data Type
(GADT\todo{cite something?}) parameterised by a list $ts$ of the types
in the union.
%
This GADT has exactly one constructor that stores
%
a tag (an \texttt{Int} meant to be understood as the index of the type
of interest in $ts$)
%
together with a value of an arbitrary type.
%
A simplified\footnote{
  Unlike Kiselyov and Ishii's original Haskell code, this version of
  \texttt{Union} is specialised to types rather than type constructors,
  and we have stripped annotations concerned with strictness or memory
  layout.
  }
version of the definition is visible in figure \ref{fig:openunion}.

\begin{figure}[h]
\begin{minted}{Haskell}
  data Union (ts :: [*]) where
    Union :: Int -> t -> Union ts
\end{minted}
\caption{A Simplified Definition of \texttt{Union} in Haskell}
\label{fig:openunion}
\end{figure}

The \texttt{Union} data constructor is not exported so that users may only
craft and manipulate elements of the open union type via the functions
provided by the library authors.
%
A library-wide invariant relying on strong assumptions about the host
language (namely global uniqueness of typeclass instances) informally
guarantees that users of the library will only ever be in possession
of elements of the union where the type of the value stored is equal
to that pointed at by the \texttt{Int} tag.
%
This justifies their widespread use of unsafe coercions when unpacking values of
the union type.

In Idris2 however we can take advantage of \emph{dependent} types to
implement a safe version of \texttt{Union} where the invariant between
the stored tag,
the type of the value,
and the list of types given as a parameter
is made explicit.
Additionally, we can take advantage of \emph{quantitative} types
to ensure that this invariant will be runtime irrelevant
thus obtaining a safe, proven correct
\emph{and} just as efficient version of the library.

The key element in this safe implementation is the definition of a three place
relation connecting
a value of a given type,
a list of values of that same type,
and a natural number
formalising the idea that the natural number is the position at which the value
is present in the list.
%
We call this three-place relation \texttt{AtIndex} and give its definition in
figure~\ref{fig:atindexrel}.

\begin{figure}[h]
\begin{minted}{Idris}
  data AtIndex : ty -> List ty -> Nat -> Type where
    Z :                   AtIndex a (a :: as) Z
    S : AtIndex a as n -> AtIndex a (b :: as) (S n)
\end{minted}
\caption{\texttt{AtIndex t ts n} specifies that \texttt{t}
  is in position \texttt{n} in \texttt{ts}}
\label{fig:atindexrel}
\end{figure}

The constructor \texttt{Z} builds a proof that the natural number \texttt{Z}
points at the head of the list by ensuring that the value of interest
is equal to it. This is expressed in Idris2 by using the same variable
\texttt{a} for both the value and the head.
%
The constructor \texttt{S} builds a proof that a successor-headed
natural number points at a value in a cons-headed list provided that the
predecessor of the number points to it in the list's tail.

We include below in~\cref{fig:atindexexample} a proof that the third
element of the list being considered
(i.e. the one whose index is $2$) is the type of natural number.

\begin{figure}[h]
\begin{minted}{Idris}
natIndex : AtIndex Nat [Int, Double, Nat, List ()] 2
natIndex = S (S Z)
\end{minted}
\caption{\texttt{Nat} is at index 2 in the given list}
\label{fig:atindexexample}
\end{figure}

\begin{remark}[Efficiency and Unary Naturals]
Readers may be concerned by our use of unary natural numbers.
Idris2 optimises natural numbers to efficient GMP-style arbitrary precision
integers so our tag will not in fact be a unary number at runtime.
This is still slightly less compact than a mere \texttt{Int}.
To save this little bit of extra memory would require a more complex
encoding enforcing that the index is always bounded. This is outside the
scope of this paper.
\end{remark}

We can use this relation to define a safe version of \texttt{Union} explicitly
enforcing the invariant between the \texttt{Nat} tag, the type of the value
stored and the list of types used as a parameter. This definition is presented
in figure~\ref{fig:safeopenunion}. The \texttt{AtIndex} argument is marked with
a \texttt{0} meaning that it is runtime irrelevant. Idris2 will ensure that we
never rely on this argument for runtime relevant information and it will
automatically erase it at compilation time.

\begin{figure}[h]
\begin{minted}{Idris}
  data Union : (ts : List Type) -> Type where
    MkUnion : (k : Nat) -> (0 _ : AtIndex t ts k) -> t -> Union ts
\end{minted}
\caption{A safe version of \texttt{Union}}
\label{fig:safeopenunion}
\end{figure}

We can reuse the proof we gave in the \texttt{AtIndex} example
in~\cref{fig:atindexexample} to produce an injection function
taking a natural number and returning a member of the union of
\texttt{Int}, \texttt{Double}, \texttt{Nat}, and \texttt{List Unit}.

\begin{figure}[h]
\begin{minted}{Idris}
natInjection : Nat -> Union [Int, Double, Nat, List ()]
natInjection = MkUnion 2 natIndex
\end{minted}
\caption{Using \texttt{natIndex} to inject a \texttt{Nat} in the given \texttt{Union}}
\label{fig:unionexexample}
\end{figure}

We can write a library similar to Kiselyov and Ishii's except that internally we
never have to use an unsafe coercion: our values are statically known to have
the correct type thanks to the proofs that are attached to them.
%
We still rely on typeclass resolution to automatically check that a type
belongs to a union.
%
But we do not need to assume global uniqueness because our typeclasses are
producing evidence of their claims and thus their instances can be trusted
even if they have been added by someone other than the library writers.

This short example will hopefully have convinced the reader that Quantitative
Type Theory can be a convenient way to remove unsafe code fragments
in favour of equivalent but safe replacements that make some structural
invariants explicit.

This refactoring also opens the door to giving more power to users:
%
when the invariants are informal users have to be treated in
an adversarial manner, assuming they may break crucial assumptions at any time
%
but when the invariants are made explicit, users can be given free reign
to access the library's internals as they will have to prove their modifications
correct.

\section{A Tree in Disguise}
\label{sec:treearray}

In this section we are going to build on our observation that we can combine
a value with an efficient runtime representation
together with a runtime irrelevant view that enforces crucial invariants
to obtain a correct by construction and efficient library.
\todo{cite ``view''}

As we have noticed earlier, a set represented as a sorted array is a balanced
binary search tree in disguise. We can make this formal by defining a tree
indexed over an array similarly to how \texttt{AtIndex} was a membership
proof indexed over a natural number.

In this instance however we are not dealing
with a pure value anymore: reading from the array is an effectful operation that
comes with very few guarantees from the type system's point of view.
To be able to prove anything at all we are going to need to make our
assumptions explicit, using unsafe primitives to provide a richly typed
interface to low level operations.

\subsection{Interfacing with unsafe code}

Our first object of interest is the array that we are going to use as the
runtime representation of our set. We decide that our set will be immutable
and correspondingly pick a read-only array as its runtime representation.

In order to connect the values contained in the nodes of our binary search tree
to those stored in the array, we introduce the \texttt{ValueAt} relation
defined in \cref{fig:valueat}.
A value of type \texttt{ValueAt arr i v} is to be understood as a proof that
if we were to read the value at index \texttt{i} in the array \texttt{arr} we
would get \texttt{v}.

\begin{figure}[h]
  \begin{minted}{Idris}
    data ValueAt : Array a -> Int -> a -> Type where
      MkValueAt : ValueAt arr i v
  \end{minted}
  \caption{Specifying a value at a given index}
  \label{fig:valueat}
\end{figure}

This definition may look surprising: it seems that, using the constructor
\texttt{MkValueAt}, users could manufacture arbitrary proofs. In practice we
do not export the constructor and the only way for users to obtain proofs of
\texttt{ValueAt} will be via the (trusted) initialisation function we provide
as part of our library for read only arrays.

\begin{figure}[h]
  \begin{minted}{Idris}
    initialise : (size : PosInt) ->
                 (f : (i : Int) -> InRange size i -> a) ->
                 IO (Subset (Array a) (IsTabulationOf f))
  \end{minted}
\end{figure}

\begin{figure}[h]
  \begin{minted}{Idris}
    IsTabulationOf : ((i : Int) -> InRange size i -> a) ->
                     Array a -> Type
    IsTabulationOf f arr = (i : Int) -> (prf : InRange size i) ->
                           ValueAt arr i (f i prf))
  \end{minted}
\end{figure}



The array is read only so it should be safe to assume that it is not possible
to get two \texttt{ValueAt} proofs relating the same index to different values.
The only way for us to build such a proof is to use an unsafe primitive
\texttt{believe\_me} to coerce an equality proof.

\begin{figure}[h]
  \begin{minted}{Idris}
    uniqueValueAt : ValueAt arr i v -> ValueAt arr i w -> v === w
    uniqueValueAt = believe_me (the (v === v) Refl)
  \end{minted}
  \caption{Assuming that said values are unique}
\end{figure}


\section{Related work}

\paragraph{LoCal}

% \paragraph{Ornaments} \texttt{AtIndex} is the algebraic ornament of a
% membership proof by the position in the list it implicitly describes.
% \todo{cite algebraic ornament}
% %% Is this relevant? Conversely Nat is the erasure of a membership proof
% %% once the forced indices have been dropped.


\todo{Check Bedrock}
\todo{Check ghosts of departed proofs}
\todo{Check liquid Haskell}
\todo{Discuss Monnier's work}

\bibliography{pearl}

\end{document}
