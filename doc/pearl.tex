\documentclass{article}

\usepackage{todonotes}

\usepackage[authoryear]{natbib}
\bibliographystyle{apalike}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}

% grab parts of other files
\usepackage{catchfilebetweentags}
\input{robust-catch}

% tree diagrams
\usepackage{tikz}
\usetikzlibrary{matrix}

% algorithm
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}


\usepackage{cleveref}

\title{Certified Binary Search in a Read-Only Array}
\author{Guillaume ALLAIS}

\begin{document}
\maketitle

\begin{abstract}
  Reifying the control flow of imperative programs as data structures allows
  functional programmers to implement common algorithms by an obviously
  terminating structural recursion. Additionally, this inductive structure
  can be decorated with invariants allowing the procedure to be proven correct.

  This ease of definition and certification comes at a cost because the
  functional data structure is typically not stored as efficiently in
  memory as the imperative counterpart it replaces.

  In this paper we use quantitative type theory as implemented in Idris 2 to
  combine an efficient runtime representation of the data with a convenient
  compile time inductive view of it to use for ease of definition and
  certification.
\end{abstract}

\section{Introduction}

One of the strengths of functional programming is the ability to reify common
control flow structures as inductive types and replace the corresponding ad-hoc
iteration constructs by recursive functions consuming these inductively defined
values.

This is best exemplified by the \texttt{forM} construct which provides users with
a convenient replacement for a \texttt{for} loop iterating over a range of values.
Instead of writing \texttt{for(i = 0; i <= 10; i=i+2) body} to execute \texttt{body}
with \texttt{i} taking the successive values $0$, $2$, etc. up to $10$, the user
can first write \texttt{[0,2..10]} to generate a list corresponding to the range
of interest and then iterate over it with \texttt{forM} thus getting the equivalent
program: \texttt{forM [0,2..10] \$ \textbackslash i -> body}.

Replacing equals for equals is of course not the most exciting. A first comparative
advantage of the functional approach is the fact that any function or system call
producing a list of values (not just integers!) can now become a source of control
flow.
%
The second and perhaps more important one comes from the fact that these constructs
can then be extended to work with a whole \emph{class} of inductive types thus
generalising the original iteration principle. In Haskell's standard library,
\texttt{forM} will for instance work over not just lists but more generally
any \texttt{Traversable} type~\citep{DBLP:journals/jfp/McbrideP08},
that is to say any finitely branching tree-like structure.

This generalisation can unfortunately come at a cost. If Haskell's optimiser is
good enough to generate tight loops for programs similar to the one above, it
will not be able to avoid the extra allocations and pointer chasing introduced by
the use of an inductive data type in all cases. We will consider one such case in
\cref{example:search}.

The work on LoCal, a functional language whose types carry information about the
runtime data-layout of the recursive structures being traversed, attempts to solve
that problem. The careful typing rules ensure the user-written programs manipulating
inductive data can be run directly on its serialised form.
%
In it, \citet{DBLP:conf/pldi/VollmerKRS0N19} warn against the explicit manipulation
of pointers. This justifies their approach: adding built-in compiler support.

\begin{quote}
  Cursors need to be manipulated carefully to visit the necessary portions
  of the buffer skipping over the sections that are not needed and read out
  the appropriate data, all without the safety net of a type checker. Hence,
  writing code to work directly on the serialized data can be tedious and
  error-prone.
\end{quote}

As we are going to see in this paper, from the point of view of the dependently
typed programmer this special built-in support is redundant.
%
In the same way that data-generic programming is usual programming over a closed
universe of descriptions of data types~\citep{DBLP:conf/ifip2-1/AltenkirchM02},
programming over serialised data is usual programming over data that is aware
of its serialisation format.

\section{Search by dichotomy}


\subsection{Search, by example}
\label{example:search}

\begin{figure}
  \center
  \ExecuteMetaData[tree.tex]{search11}
  \caption{Tracing the binary search for 11}
\end{figure}

\begin{figure}
  \center
  \ExecuteMetaData[tree.tex]{search31}
  \caption{Tracing the binary search for 31}
\end{figure}

\subsection{Search, imperatively}

\input{algo}

\subsection{The computation's underlying structure}


Coming back to our running example, if we take the time to trace all the possible
execution paths of binary search on our read only array we see a tree revealed
(cf. \cref{fig:searchall}).
This is the underlying structure of the binary search computation: a binary search
tree! \todo{Be precise: AVL?}

\begin{figure}
  \center
  \ExecuteMetaData[tree.tex]{searchall}
  \caption{Tracing all possible binary searches}
  \label{fig:searchall}
\end{figure}

This suggests a straightforward avenue for formalisation: ditch the array and
use the underlying binary search tree instead. This has the added benefit of
replacing an induction over the size of the subarray the search is performed on
into a purely structural one on the tree.

However this would have adverse consequences in terms of performance: instead of
a compact representation using a contiguous array, we would have a tree whose nodes
comprise of a value and two pointers to its children.

Can we have the best of both worlds? A compact runtime representation using an
array and an obviously terminating recursive function amenable to verification.

\section{}


\bibliography{pearl}

\end{document}
