\documentclass{article}

\usepackage{todonotes}

\usepackage[authoryear]{natbib}
\bibliographystyle{apalike}

\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}

% grab parts of other files
\usepackage{catchfilebetweentags}
\input{robust-catch}

% tree diagrams
\usepackage{tikz}
\usetikzlibrary{matrix}

% algorithm
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

%haskell
\usepackage{minted}

% parens around citeyear
\newcommand{\parenciteyear}[1]{(\citeyear{#1})}


\usepackage{cleveref}

\title{Certified Binary Search in a Read-Only Array}
\author{Guillaume ALLAIS}

\begin{document}
\maketitle

\begin{abstract}
  Reifying the control flow of imperative programs as data structures allows
  functional programmers to implement common algorithms by an obviously
  terminating structural recursion. Additionally, this inductive structure
  can be decorated with invariants allowing the procedure to be proven correct.

  This ease of definition and certification comes at a cost because the
  functional data structure is typically not stored as efficiently in
  memory as the imperative counterpart it replaces.

  In this paper we use quantitative type theory as implemented in Idris2 to
  combine an efficient runtime representation of the data with a convenient
  compile time inductive view of it to use for ease of definition and
  certification.
\end{abstract}

\section{Introduction}

One of the strengths of functional programming is the ability to reify common
control flow structures as inductive types and replace the corresponding ad-hoc
iteration constructs by recursive functions consuming these inductively defined
values.

This is perhaps best exemplified by the \texttt{forM} construct which provides
users with a convenient replacement for a \texttt{for} loop iterating over a
range of values.
%
Instead of writing \texttt{for(i = 0; i <= 10; i=i+2) body} to execute
\texttt{body} with \texttt{i} taking the successive values $0$, $2$, etc. up to
$10$, the user can first write \texttt{[0,2..10]} to generate a list
corresponding to the range of interest and then iterate over it with
\texttt{forM} thus getting the equivalent program
\texttt{forM [0,2..10] \$ \textbackslash i -> body}.

Replacing equals for equals is of course not the most exciting.
%
A first comparative advantage of the functional approach is the fact that any
function or system call producing a list of values (not just integers!) can now
become a source of control flow.
%
The second and perhaps more important one comes from the fact that these
constructs can then be extended to work with a whole \emph{class} of inductive
types thus generalising the original iteration principle.
%
In Haskell's standard library, \texttt{forM} will for instance work over not
just lists but more generally any \texttt{Traversable}
type~\citep{DBLP:journals/jfp/McbrideP08},
that is to say any finitely branching tree-like structure.

This generalisation can unfortunately come at a cost. If Haskell's optimiser is
good enough to generate tight loops for programs similar to the one above, it
will not be able to avoid the extra allocations and pointer chasing introduced by
the use of an inductive data type in all cases. We will consider one such case in
\cref{example:search}.

The work on LoCal, a functional language whose types carry information about the
runtime data-layout of the recursive structures being traversed, attempts to solve
that problem. The careful typing rules ensure the user-written programs manipulating
inductive data can be run directly on its serialised form.
%
In it, \citet{DBLP:conf/pldi/VollmerKRS0N19} warn against the explicit manipulation
of pointers (what they call Cursor is essentially a type-informed offset of
a pointer)\todo{double check}.
%
This justifies their approach: adding built-in compiler support.

\begin{quote}
  Cursors need to be manipulated carefully to visit the necessary portions
  of the buffer skipping over the sections that are not needed and read out
  the appropriate data, all without the safety net of a type checker. Hence,
  writing code to work directly on the serialized data can be tedious and
  error-prone.
\end{quote}

As we are going to see in this paper, from the point of view of the dependently
typed programmer this special built-in support is redundant.
%
In the same way that data-generic programming is usual programming over a closed
universe of descriptions of data types~\citep{DBLP:conf/ifip2-1/AltenkirchM02},
programming over serialised data is usual programming over data that is aware
of its serialisation format.

\subsection{Roadmap}\todo{expand}

In this pearl we will focus on a specific algorithm: binary search in a
sorted read-only array. This is typically used to efficiently check whether
a given value belongs to a set.

\paragraph{Section \ref{sec:reconstruct}} is dedicated to rediscovering the
underlying structure of search by dichotomy. It leads us to the corresponding
functional abstraction using a binary search tree.

\paragraph{Section \ref{sec:qtt}} is a quick introduction to the quantitative
type theory underpinning Idris2 and the notion of runtime irrelevance that it
provides.

\paragraph{Section \ref{sec:treearray}} is the core of our pearl: the
runtime irrelevant inductive structure that allows us to see a read-only array
as a tree in disguise.
%
All of the invariant-enforcing information the runtime irrelevant structure
carries allows us to prove a logarithmic \emph{decidability} principle for
array membership.


\section{Search by dichotomy}
\label{sec:reconstruct}

Binary search is a classic example of an algorithm offering an asymptotic
improvement over a naïve approach, under the assumption that the input data
is structured.
%
Provided a set represented as an array that is known to be sorted, it is
possible to test set membership in logarithmic time instead of taking linear
time to compare the value of interest to each value in the set.

The key observation underpinning this algorithm is that, by virtue of the array
being sorted, the value in the middle of any subarray divides it in two parts:
the left one, only containing strictly smaller values,
and the right one, only containing strictly larger ones.
%
Using this knowledge, at each step of the procedure we can eliminate half of
the remaining candidates. Comparing the value in the middle of the subarray
we are focused on with our value of interest, we can
either stop if we have successfully found our target
or focus on the left or the right half of the subarray depending on whether
our target is smaller or larger than the value we just read.

Let us look at a couple of executions of this search algorithm on a set
represented as a sorted array.

\subsection{Search, by example}
\label{example:search}

In figure~\ref{fig:searchsuccess} we are looking for
the value 11 in a sorted array which happens to contain the first
ten prime numbers.
%
We start by reading the value in the middle of the whole array, find out it is
13 which is larger than 11 and thus focus on the left half of the array for the
next step.
%
We then read the value in the middle of this left half, obtain 5
which is smaller than 11 and thus shift our focus to the right of 5, that is
to say on the second quarter of the whole array.
%
After reading 7, we shift once more our focus to the right and finally manage
to find our target. The search for 11 was successful.

\begin{figure}[h]
  \center
  \ExecuteMetaData[tree.tex]{search11}
  \caption{Tracing the binary search for 11}
  \label{fig:searchsuccess}
\end{figure}

If we had been looking for the value 31 instead, we would have obtained the
unsuccessful trace shown in figure~\ref{fig:searchfail}. We start from the
middle of the array and repeatedly shift our focus to the right because we
keep reading numbers smaller than our target. We eventually reach the end of
the array and can declare that 31 is not a member of the set.


\begin{figure}[h]
  \center
  \ExecuteMetaData[tree.tex]{search31}
  \caption{Tracing the binary search for 31}
  \label{fig:searchfail}
\end{figure}

\subsection{Search, imperatively}


\input{algo}

\subsection{The computation's underlying structure}

This listing gives us a formal definition of the algorithm. It should be
enough to prove it terminating and correct under the assumption that the
input array is sorted.
%
However it has not shone any light on the structure of the computation.
If a list of values is the essence of a \texttt{for} loop,
what is that of a search by dichotomy?
%
This can be answered by going back to our running example and taking the
time to observe the behaviour of binary search tree.

In figure \ref{fig:searchall} we trace all of the possible execution paths
and see a tree revealed.
%
This is the underlying structure of the binary search computation: a binary
search tree! Additionally, because the procedure is guaranteed to halve the
number of candidates at each step, we know that this binary search tree is
balanced.\todo{Be precise: AVL?}

\begin{figure}[h]
  \center
  \ExecuteMetaData[tree.tex]{searchall}
  \caption{Tracing all possible binary searches}
  \label{fig:searchall}
\end{figure}

This suggests a straightforward avenue for implementation in a functional
language or formalisation in a proof assistant: ditch the array and use the
underlying binary search tree instead!
%
This has the added benefit of replacing an unwieldy induction over the size
of the subarray in focus in favour of a purely structural one on the tree.

Additionally, in a dependently typed language we can annotate the tree with
invariants thus guaranteeing that the values are indeed sorted. This empowers
us to not only write a search procedure but also prove that it decides the
set membership problem.

However this would have adverse consequences in terms of performance: instead of
a compact representation using a contiguous array, we now have a tree whose
nodes comprise of a value and two pointers to its children.

Can we have the best of both worlds? A compact runtime representation using an
array and an obviously terminating recursive decision procedure?
%
We will decidedly answer yes in section~\ref{sec:treearray} thanks to the tools
we are going to introduce now.


\section{QTT and Runtime Irrelevance}
\label{sec:qtt}

\subsection{Example: a Safe Open Union Type}

In their work on extensible
effects~\parenciteyear{DBLP:conf/haskell/KiselyovI15}
Kiselyov and Ishii use an open union type to efficiently represent the
sum of all of the effects available in a given computation.

Internally this open union type is a Generalised Algebraic Data Type
(GADT\todo{cite something?}) parameterised by a list $ts$ of the types
in the union.
%
This GADT has exactly one constructor that stores
%
a tag (an \texttt{Int} meant to be understood as the index of the type
of interest in $ts$)
%
together with a value of an arbitrary type.
%
A simplified\footnote{
  Unlike Kiselyov and Ishii's original Haskell code, this version of
  \texttt{Union} is specialised to types rather than type constructors,
  and we have stripped annotations concerned with strictness or memory
  layout.
  }
version of the definition is visible in figure \ref{fig:openunion}.

\begin{figure}[h]
\begin{minted}{Haskell}
  data Union (ts :: [*]) where
    Union :: Int -> t -> Union ts
\end{minted}
\caption{A Simplified Definition of \texttt{Union}}
\label{fig:openunion}
\end{figure}

The \texttt{Union} data constructor is not exported so that users may only
craft and manipulate elements of the open union type via the functions
provided by the library authors.
%
A library-wide invariant relying on strong assumptions about the host
language (namely global uniqueness of typeclass instances) informally
guarantees that users of the library will only ever be in possession
of elements of the union where the type of the value stored is equal
to that pointed at by the \texttt{Int} index.
%
This justifies their widespread use of unsafe coercions when unpacking values of
the union type.

In Idris2 however we can take advantage of \emph{dependent} types to
implement a safe version of \texttt{Union} where the invariant between
the stored tag,
the type of the value,
and the list of types given as a parameter
is made explicit.
Additionally, we can take advantage of \emph{quantitative} types
to ensure that this invariant will be runtime irrelevant
thus obtaining a safe, proven correct
\emph{and} just as efficient version of the library.

The key element in this safe implementation is the definition of a three place
relation connecting
a value of a given type,
a list of values of that same type,
and a natural number
formalising the idea that the natural number is the position at which the value
is present in the list.
%
We call this three-place relation \texttt{AtIndex} and give its definition in
figure~\ref{fig:atindexrel}.

\begin{figure}[h]
\begin{minted}{Idris}
  data AtIndex : ty -> List ty -> Nat -> Type where
    Z :                   AtIndex a (a :: as) Z
    S : AtIndex a as n -> AtIndex a (b :: as) (S n)
\end{minted}
\caption{\texttt{AtIndex t ts n} specifies that \texttt{t}
  is in position \texttt{n} in \texttt{ts}}
\label{fig:atindexrel}
\end{figure}

The constructor \texttt{Z} builds a proof that the natural number \texttt{Z}
points at the head of the list by ensuring that the value of interest
is equal to it. This is expressed in Idris2 by using the same variable
\texttt{a} for both the value and the head.
%
The constructor \texttt{S} builds a proof that a successor-headed
natural number points at a value in a cons-headed list provided that the
predecessor of the number points to it in the list's tail.

\paragraph{Remark}
Readers may be concerned by our use of unary natural numbers.
Idris2 optimises natural numbers to efficient GMP-style arbitrary precision
integers so our tag will not in fact be a unary number at runtime.
This is still slightly less compact than a mere \texttt{Int}.
To save this little bit of extra memory would require a more complex
encoding enforcing that the index is always bounded. This is outside the
scope of this paper.

We can use this relation to define a safe version of \texttt{Union} explicitly
enforcing the invariant between the \texttt{Nat} tag, the type of the value
stored and the list of types used as a parameter. This definition is presented
in figure~\ref{fig:safeopenunion}. The \texttt{AtIndex} argument is marked with
a \texttt{0} meaning that it is runtime irrelevant. Idris2 will ensure that we
never rely on this argument for runtime relevant information and it will
automatically erase it at compilation time.

\begin{figure}[h]
\begin{minted}{Idris}
  data Union : (ts : List Type) -> Type where
    MkUnion : (k : Nat) -> (0 _ : AtIndex t ts k) -> t -> Union ts
\end{minted}
\caption{A safe version of \texttt{Union}}
\label{fig:safeopenunion}
\end{figure}

We can write a library similar to Kiselyov and Ishii's except that internally we
never have to use an unsafe coercion: our values are statically known to have
the correct type thanks to the proofs that are attached to them.
%
We still rely on typeclass resolution to automatically check that a type
belongs to a union.
%
But we do not need to assume global uniqueness because our typeclasses are
producing evidence of their claims and thus can be trusted even if they have
been added by someone other than the library writers.

This short example will hopefully have convinced the reader that Quantitative
Type Theory can be a convenient way to remove unsafe code fragments
in favour of equivalent but safe replacements that make some structural
invariants explicit.

\section{A Tree in Disguise}
\label{sec:treearray}



\section{Related work}

\paragraph{LoCal}

\todo{Check Bedrock}
\todo{Check ghosts of departed proofs}
\todo{Check liquid Haskell}

\bibliography{pearl}

\end{document}
